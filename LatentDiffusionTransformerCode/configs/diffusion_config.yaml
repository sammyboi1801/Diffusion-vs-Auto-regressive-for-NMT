# Configuration for the LLaDA-style Diffusion Model

model_params:
  dim: 1024
  n_layers: 6
  n_heads: 6
  vocab_size: 119547 # From bert-base-multilingual-cased
  max_len: 256
  mask_token_id: 103   # From bert-base-multilingual-cased
  pad_token_id: 0      # From bert-base-multilingual-casedb

data:
  # Path to a text dataset (e.g., a CSV file)
  path: 'LatentDiffusionTransformerCode/data/en-fr.csv'
  format: 'csv'

training:
  batch_size: 32
  num_epochs: 20
  learning_rate: 0.0001
  optimizer: 'AdamW'

inference:
  # Number of iterative denoising steps for generation
  steps: 40
  # Number of new tokens to generate
  gen_length: 60
