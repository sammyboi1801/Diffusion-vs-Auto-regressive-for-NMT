{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4408e8a6",
      "metadata": {
        "id": "4408e8a6"
      },
      "source": [
        "# Autoregressive vs. Masked Diffusion Transformers for Translation (En - Fr)\n",
        "\n",
        "### 1. Introduction\n",
        "Autoregressive transformer models have become the dominant paradigm for text generation, achieving state-of-the-art performance across tasks like translation and summarization. These models, such as GPT-2 and GPT-3, generate text sequentially by predicting one token at a time from left to right, conditioning each prediction on all previous tokens.\n",
        "\n",
        "While this approach produces highly coherent text, it suffers from an inherent sequential bottleneck that prevents parallelization and results in slow inference times. Recently, diffusion models have emerged as a promising alternative, generating all tokens in parallel through an iterative denoising process.\n",
        "\n",
        "In this notebook, we implement and compare these two paradigms using identical 44M-parameter architectures trained on the OPUS Books English-to-French dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4e273d0",
      "metadata": {
        "id": "b4e273d0"
      },
      "source": [
        "#### 1.1 Environment set-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b7974c7-b1b0-4250-b563-40584049e2df",
      "metadata": {
        "id": "5b7974c7-b1b0-4250-b563-40584049e2df",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch tqdm numpy pandas scikit-learn\n",
        "!pip install sacrebleu rouge-score bert-score\n",
        "!pip install datasets tokenizers\n",
        "!pip install hf_transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78f20896-ac4a-4185-81b4-ff040822e1d7",
      "metadata": {
        "id": "78f20896-ac4a-4185-81b4-ff040822e1d7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "import json\n",
        "import sacrebleu\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
        "from datasets import load_dataset\n",
        "from tqdm.auto import tqdm\n",
        "import collections\n",
        "\n",
        "# Standardizing Seeds for Reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fce7de8d",
      "metadata": {
        "id": "fce7de8d"
      },
      "source": [
        "### Configuration and Data\n",
        "\n",
        "\n",
        "#### 2.1 Dataset Description\n",
        "We use the OPUS Books dataset for English-to-French machine translation, comprising 127,000 sentence pairs. The English sentences average 15.2 tokens in length, while French translations average 16.8 tokens.\n",
        "\n",
        "\n",
        "#### 2.2 Tokenization Strategy\n",
        "We developed a custom Byte Pair Encoding (BPE) tokenizer with a vocabulary size of 20,000 tokens, trained jointly on both English and French text. This balances expressiveness with computational efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd1c29c1-2562-4b8d-a2f8-648ba91c26e3",
      "metadata": {
        "id": "dd1c29c1-2562-4b8d-a2f8-648ba91c26e3"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    # Data & Tokenizer\n",
        "    tokenizer_path = \"custom_opus_bpe.json\"\n",
        "    max_input_len = 128\n",
        "    max_output_len = 128\n",
        "\n",
        "    # Model Architecture\n",
        "    d_model = 512\n",
        "    n_heads = 8\n",
        "    n_layers = 8\n",
        "    d_ff = 2048\n",
        "    dropout = 0.1\n",
        "    activation = \"silu\"\n",
        "    vocab_size = 20000\n",
        "\n",
        "    pad_token_id = None\n",
        "    sep_token_id = None\n",
        "    mask_token_id = None\n",
        "    eos_token_id = None\n",
        "\n",
        "    # Training\n",
        "    batch_size = 16\n",
        "    accumulate_grad_batches = 4\n",
        "    learning_rate = 1e-4\n",
        "    weight_decay = 0.01\n",
        "    warmup_steps = 1000\n",
        "    epochs = 50\n",
        "    clip_grad = 1.0\n",
        "\n",
        "    # Diffusion Specifics\n",
        "    timesteps = 100\n",
        "    sampling_steps = 50\n",
        "\n",
        "    # Logging\n",
        "    log_dir = \"logs\"\n",
        "    sample_dir = \"samples\"\n",
        "    checkpoint_dir = \"checkpoints\"\n",
        "    save_top_k = 3\n",
        "\n",
        "    def to_dict(self):\n",
        "        return {k: v for k, v in Config.__dict__.items() if not k.startswith('__') and not callable(v)}\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(config.log_dir, exist_ok=True)\n",
        "os.makedirs(config.sample_dir, exist_ok=True)\n",
        "os.makedirs(config.checkpoint_dir, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ec6152f-24f2-4424-b985-de89553fea33",
      "metadata": {
        "id": "1ec6152f-24f2-4424-b985-de89553fea33"
      },
      "outputs": [],
      "source": [
        "# 1. Load the OPUS Books dataset (English-French subset)\n",
        "print(\"Loading OPUS Books (en-fr)...\")\n",
        "dataset = load_dataset(\"opus_books\", \"en-fr\", split=\"train\")\n",
        "\n",
        "# 2. Format it for your specific pipeline\n",
        "data = []\n",
        "\n",
        "print(\"Formatting dataset...\")\n",
        "for item in dataset:\n",
        "    question = item['translation']['en']\n",
        "    answer = item['translation']['fr']\n",
        "    data.append({'question': question, 'answer': answer})\n",
        "\n",
        "# 3. Save to CSV\n",
        "data_path = 'french_dataset.csv'\n",
        "df_fr = pd.DataFrame(data)\n",
        "df_fr.to_csv(data_path, index=False)\n",
        "print(f\"Saved {data_path} with {len(df_fr)} pairs.\")\n",
        "\n",
        "# 4. Train Custom BPE Tokenizer\n",
        "print(\"Training Custom BPE Tokenizer...\")\n",
        "\n",
        "# Initialize BPE tokenizer\n",
        "tokenizer_obj = Tokenizer(models.BPE())\n",
        "tokenizer_obj.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "\n",
        "# Define special tokens\n",
        "# We include <sep> and <mask> specifically for our architecture\n",
        "special_tokens = [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\", \"<sep>\", \"<mask>\"]\n",
        "\n",
        "trainer = trainers.BpeTrainer(\n",
        "    vocab_size=20000,\n",
        "    special_tokens=special_tokens,\n",
        "    min_frequency=2\n",
        ")\n",
        "\n",
        "# Train on the questions and answers\n",
        "# We stream from the dataframe to avoid massive memory usage if data is large\n",
        "def batch_iterator():\n",
        "    for i in range(0, len(df_fr), 1000):\n",
        "        yield df_fr[i : i + 1000][\"question\"].astype(str).tolist()\n",
        "        yield df_fr[i : i + 1000][\"answer\"].astype(str).tolist()\n",
        "\n",
        "tokenizer_obj.train_from_iterator(batch_iterator(), trainer=trainer)\n",
        "\n",
        "# Save the tokenizer JSON\n",
        "tokenizer_obj.save(config.tokenizer_path)\n",
        "print(f\"Tokenizer saved to {config.tokenizer_path} with vocab size {tokenizer_obj.get_vocab_size()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc7753c1",
      "metadata": {
        "id": "dc7753c1"
      },
      "source": [
        "#### 2.3 Dataset Class and Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a717caa-c198-4c78-af08-1d605ebae9e6",
      "metadata": {
        "id": "0a717caa-c198-4c78-af08-1d605ebae9e6"
      },
      "outputs": [],
      "source": [
        "class PreTokenizedDataset(Dataset):\n",
        "    def __init__(self, data_path, tokenizer, max_input, max_output):\n",
        "        self.examples = []\n",
        "        df = pd.read_csv(data_path, on_bad_lines='skip', engine='python').dropna(subset=['question', 'answer'])\n",
        "\n",
        "        # Batch tokenize using tokenizer.batch_encode_plus (Much faster than loops)\n",
        "        print(\"Pre-tokenizing dataset... this takes a moment but speeds up training.\")\n",
        "\n",
        "        questions = df['question'].astype(str).tolist()\n",
        "        answers = df['answer'].astype(str).tolist()\n",
        "\n",
        "        q_enc = tokenizer(questions, truncation=True, max_length=max_input, add_special_tokens=False)\n",
        "        a_enc = tokenizer(answers, truncation=True, max_length=max_output, add_special_tokens=False)\n",
        "\n",
        "        sep_id = tokenizer.convert_tokens_to_ids('<sep>')\n",
        "        eos_id = tokenizer.eos_token_id\n",
        "\n",
        "        for q, a in zip(q_enc['input_ids'], a_enc['input_ids']):\n",
        "            # Construct: Q + SEP + A + EOS\n",
        "            full_ids = q + [sep_id] + a + [eos_id]\n",
        "            # Truncate\n",
        "            if len(full_ids) > (max_input + max_output + 1):\n",
        "                full_ids = full_ids[:(max_input + max_output + 1)]\n",
        "            self.examples.append(torch.tensor(full_ids, dtype=torch.long))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.examples[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e4f5e50-7889-47ec-b695-267a06ab3939",
      "metadata": {
        "id": "4e4f5e50-7889-47ec-b695-267a06ab3939"
      },
      "outputs": [],
      "source": [
        "# --- Tokenizer Setup (Custom BPE) ---\n",
        "# We wrap the custom JSON in a Transformers-compatible class\n",
        "tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_file=config.tokenizer_path,\n",
        "    bos_token=\"<s>\",\n",
        "    eos_token=\"</s>\",\n",
        "    unk_token=\"<unk>\",\n",
        "    pad_token=\"<pad>\",\n",
        "    mask_token=\"<mask>\"\n",
        ")\n",
        "\n",
        "# Add sep token manually as it's not a standard HF attribute\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': ['<sep>']})\n",
        "\n",
        "# Update Config with IDs\n",
        "config.vocab_size = len(tokenizer)\n",
        "config.pad_token_id = tokenizer.pad_token_id\n",
        "config.sep_token_id = tokenizer.convert_tokens_to_ids('<sep>')\n",
        "config.mask_token_id = tokenizer.convert_tokens_to_ids('<mask>')\n",
        "config.eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "print(f\"Vocab size: {config.vocab_size}\")\n",
        "print(f\"SEP token ID: {config.sep_token_id}\")\n",
        "print(f\"MASK token ID: {config.mask_token_id}\")\n",
        "print(f\"PAD token ID: {config.pad_token_id}\")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    # Pad sequences\n",
        "    padded_ids = pad_sequence(batch, batch_first=True, padding_value=config.pad_token_id)\n",
        "    # Create attention mask (1 for real tokens, 0 for pad)\n",
        "    attention_mask = (padded_ids != config.pad_token_id).long()\n",
        "    return padded_ids, attention_mask\n",
        "\n",
        "# Instantiate Data Loaders using the new CSV\n",
        "data_path = 'french_dataset.csv'\n",
        "\n",
        "# Load the full dataset\n",
        "# We use the PreTokenizedDataset class defined in the previous cell\n",
        "full_dataset = PreTokenizedDataset(\n",
        "    data_path, tokenizer, config.max_input_len, config.max_output_len\n",
        ")\n",
        "\n",
        "# Split Train/Val\n",
        "val_ratio = 0.05\n",
        "n_val = int(len(full_dataset) * val_ratio)\n",
        "n_train = len(full_dataset) - n_val\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "    full_dataset, [n_train, n_val]\n",
        ")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=config.batch_size, shuffle=True,\n",
        "    collate_fn=collate_fn, num_workers=2, pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=config.batch_size, shuffle=False,\n",
        "    collate_fn=collate_fn, num_workers=2, pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Training examples: {len(train_dataset)}\")\n",
        "print(f\"Validation examples: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ba1ccc9",
      "metadata": {
        "id": "0ba1ccc9"
      },
      "source": [
        "### 3. Methods: Model Architecture\n",
        "We implemented two 8-layer transformer models with identical architectures to ensure fair comparison. Both models utilize 43,992,576 parameters and learned positional embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "832e383a",
      "metadata": {
        "id": "832e383a"
      },
      "source": [
        "#### 3.1 Shared Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ea73e87-3b23-4e19-a651-451f736739cf",
      "metadata": {
        "id": "7ea73e87-3b23-4e19-a651-451f736739cf"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        var = torch.mean(x ** 2, dim=-1, keepdim=True)\n",
        "        x_normed = x * torch.rsqrt(var + self.eps)\n",
        "        return self.weight * x_normed\n",
        "\n",
        "class SwiGLU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x, gate = x.chunk(2, dim=-1)\n",
        "        return F.silu(gate) * x\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm1 = RMSNorm(d_model)\n",
        "        self.norm2 = RMSNorm(d_model)\n",
        "\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, 2 * d_ff),\n",
        "            SwiGLU(),\n",
        "            nn.Linear(d_ff, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, attn_mask=None, key_padding_mask=None):\n",
        "        residual = x\n",
        "        x_norm = self.norm1(x)\n",
        "\n",
        "        attn_out, _ = self.self_attn(\n",
        "            x_norm, x_norm, x_norm,\n",
        "            attn_mask=attn_mask,\n",
        "            key_padding_mask=key_padding_mask,\n",
        "            need_weights=False\n",
        "        )\n",
        "        x = residual + self.dropout(attn_out)\n",
        "\n",
        "        # FFN\n",
        "        residual = x\n",
        "        x_norm = self.norm2(x)\n",
        "        x = residual + self.ffn(x_norm)\n",
        "\n",
        "        return x\n",
        "\n",
        "class PositionalEmbedding(nn.Module):\n",
        "    def __init__(self, max_len, d_model):\n",
        "        super().__init__()\n",
        "        self.pe = nn.Embedding(max_len, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0)\n",
        "        return self.pe(positions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fc6a47b",
      "metadata": {
        "id": "3fc6a47b"
      },
      "source": [
        "#### 3.2 Autoregressive Model (ARM)\n",
        "The autoregressive baseline uses standard causal masking to prevent attending to future tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23f6f014-c23d-4535-9142-a440b822f310",
      "metadata": {
        "id": "23f6f014-c23d-4535-9142-a440b822f310"
      },
      "outputs": [],
      "source": [
        "class ARM(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.pos_emb = PositionalEmbedding(config.max_input_len + config.max_output_len + 10, config.d_model)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerDecoderLayer(config.d_model, config.n_heads, config.d_ff, config.dropout)\n",
        "            for _ in range(config.n_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm_final = RMSNorm(config.d_model)\n",
        "        self.output_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "        self.output_head.weight = self.token_emb.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "\n",
        "        seq_len = input_ids.size(1)\n",
        "\n",
        "        x = self.token_emb(input_ids) + self.pos_emb(input_ids)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=input_ids.device) * float('-inf'), diagonal=1)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            key_padding_mask = (attention_mask == 0)\n",
        "        else:\n",
        "            key_padding_mask = None\n",
        "\n",
        "        # Transformer Layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attn_mask=causal_mask, key_padding_mask=key_padding_mask)\n",
        "\n",
        "        x = self.norm_final(x)\n",
        "        logits = self.output_head(x)\n",
        "        return logits\n",
        "\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "973ada24",
      "metadata": {
        "id": "973ada24"
      },
      "source": [
        "#### 3.3 Masked Diffusion Model (MDM)\n",
        "Our diffusion model follows the masked discrete diffusion framework inspired by LLaDA. During training, the model learns to predict original tokens at masked positions using bidirectional attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56d7d2c0-1b77-4288-af3b-451da47afd13",
      "metadata": {
        "id": "56d7d2c0-1b77-4288-af3b-451da47afd13"
      },
      "outputs": [],
      "source": [
        "class DiffusionLLM(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_emb = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.pos_emb = PositionalEmbedding(config.max_input_len + config.max_output_len + 10, config.d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        # Re-use the exact same block structure\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerDecoderLayer(config.d_model, config.n_heads, config.d_ff, config.dropout)\n",
        "            for _ in range(config.n_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm_final = RMSNorm(config.d_model)\n",
        "        self.output_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "        # Weight tying\n",
        "        self.output_head.weight = self.token_emb.weight\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "\n",
        "        x = self.token_emb(input_ids) + self.pos_emb(input_ids)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            key_padding_mask = (attention_mask == 0)\n",
        "        else:\n",
        "            key_padding_mask = None\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, attn_mask=None, key_padding_mask=key_padding_mask)\n",
        "\n",
        "        x = self.norm_final(x)\n",
        "        logits = self.output_head(x)\n",
        "        return logits\n",
        "\n",
        "    def count_parameters(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"Re-initializing models with updated class structure...\")\n",
        "model_arm = ARM(config).to(device)\n",
        "model_diff = DiffusionLLM(config).to(device)\n",
        "\n",
        "model_arm = torch.compile(model_arm)\n",
        "model_diff = torch.compile(model_diff)\n",
        "\n",
        "params_arm = model_arm.count_parameters()\n",
        "params_diff = model_diff.count_parameters()\n",
        "\n",
        "print(f\"ARM Parameters: {params_arm:,}\")\n",
        "print(f\"Diffusion Parameters: {params_diff:,}\")\n",
        "diff_perc = abs(params_arm - params_diff) / params_arm * 100\n",
        "print(f\"Difference: {diff_perc:.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b422a2a8",
      "metadata": {
        "id": "b422a2a8"
      },
      "source": [
        "### 4. Training Mechanism"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1d78fe6",
      "metadata": {
        "id": "a1d78fe6"
      },
      "source": [
        "#### 4.1 Masking and Loss Strategy\n",
        "1) ARM: Minimizes cross-entropy loss for next-token prediction across all positions.\n",
        "2) Diffusion: Randomly masks tokens based on a linear schedule. Cross-entropy loss is computed only on masked tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ba3a4ba-4f84-47b1-a5cc-8825bf61e5cd",
      "metadata": {
        "id": "3ba3a4ba-4f84-47b1-a5cc-8825bf61e5cd"
      },
      "outputs": [],
      "source": [
        "def get_mask_ratio(t, T, schedule=\"cosine\"):\n",
        "    if schedule == \"linear\":\n",
        "        return t.float() / T\n",
        "    elif schedule == \"cosine\":\n",
        "        return torch.cos((t.float() / T) * math.pi * 0.5)\n",
        "    return t.float() / T\n",
        "\n",
        "def train_step_arm(model, batch):\n",
        "    input_ids, attention_mask = batch\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "\n",
        "    inputs = input_ids[:, :-1]\n",
        "    targets = input_ids[:, 1:]\n",
        "    att_mask = attention_mask[:, :-1]\n",
        "\n",
        "    logits = model(inputs, attention_mask=att_mask)\n",
        "\n",
        "    # Flatten\n",
        "    loss = F.cross_entropy(logits.reshape(-1, config.vocab_size), targets.reshape(-1), ignore_index=config.pad_token_id)\n",
        "    return loss\n",
        "\n",
        "def train_step_diffusion(model, batch):\n",
        "    input_ids, attention_mask = batch\n",
        "    input_ids = input_ids.to(device)\n",
        "    attention_mask = attention_mask.to(device)\n",
        "    batch_size, seq_len = input_ids.shape\n",
        "\n",
        "    t = torch.rand(batch_size, device=device)\n",
        "\n",
        "    ratio = torch.cos((t * math.pi * 0.5))\n",
        "\n",
        "    # 2. Determine which tokens can be masked\n",
        "    indices = torch.arange(seq_len, device=device).expand(batch_size, seq_len)\n",
        "\n",
        "    # Find position of SEP token (argmax returns the first index of max value)\n",
        "    sep_locs = (input_ids == config.sep_token_id).long().argmax(dim=1).unsqueeze(1)\n",
        "\n",
        "    # Mask is true if index > sep_position (Only mask the Answer part)\n",
        "    can_be_masked = indices > sep_locs\n",
        "\n",
        "    # 3. Create Random Mask Probabilities\n",
        "    probs = torch.rand(batch_size, seq_len, device=device)\n",
        "    mask_mask = (probs < ratio.unsqueeze(1)) & can_be_masked & (input_ids != config.pad_token_id)\n",
        "\n",
        "    # 4. Apply Mask\n",
        "    masked_input = input_ids.clone()\n",
        "    masked_input[mask_mask] = config.mask_token_id\n",
        "\n",
        "    # 5. Forward\n",
        "    logits = model(masked_input, attention_mask=attention_mask)\n",
        "\n",
        "    # 6. Loss Calculation\n",
        "    # We explicitly IGNORE the pad token so the model doesn't learn to predict it\n",
        "    loss = F.cross_entropy(\n",
        "        logits.view(-1, config.vocab_size),\n",
        "        input_ids.view(-1),\n",
        "        reduction='none',\n",
        "        ignore_index=config.pad_token_id\n",
        "    )\n",
        "\n",
        "    loss = loss.view(batch_size, seq_len)\n",
        "\n",
        "    # Only count loss on masked tokens\n",
        "    masked_loss = loss * mask_mask.float()\n",
        "\n",
        "    # 7. Stable Loss Aggregation\n",
        "    # Instead of dividing by t (which causes exploding gradients/collapse),\n",
        "    # we normalize by the actual count of masked tokens.\n",
        "    num_masked_tokens = mask_mask.sum() + 1e-6\n",
        "\n",
        "    return masked_loss.sum() / num_masked_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78fcaae7",
      "metadata": {
        "id": "78fcaae7"
      },
      "source": [
        "#### 4.2 Training Loop\n",
        "Both models are trained for 50 epochs using the AdamW optimizer with a learning rate of 1e-4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ec1b283-9f72-47c8-b31b-68699db1e215",
      "metadata": {
        "id": "0ec1b283-9f72-47c8-b31b-68699db1e215"
      },
      "outputs": [],
      "source": [
        "def train_model(model, model_type, train_loader, val_loader, config):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs * len(train_loader))\n",
        "\n",
        "    metrics_log = []\n",
        "\n",
        "    print(f\"Starting training for {model_type}...\")\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{config.epochs}\")\n",
        "\n",
        "        scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "        for step, batch in enumerate(pbar):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            with torch.amp.autocast('cuda'): # Casts operations to FP16 automatically\n",
        "                if model_type == \"ARM\":\n",
        "                    loss = train_step_arm(model, batch)\n",
        "                else:\n",
        "                    loss = train_step_diffusion(model, batch)\n",
        "\n",
        "                loss = loss / config.accumulate_grad_batches\n",
        "\n",
        "            # Scale the loss\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            if (step + 1) % config.accumulate_grad_batches == 0:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.clip_grad)\n",
        "\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                scheduler.step()\n",
        "\n",
        "            total_loss += loss.item() * config.accumulate_grad_batches\n",
        "            pbar.set_postfix({'loss': total_loss / (step + 1)})\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "\n",
        "        # Validation\n",
        "        val_loss = evaluate_loss(model, model_type, val_loader)\n",
        "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Save Checkpoint\n",
        "        ckpt_path = os.path.join(config.checkpoint_dir, f\"{model_type}_epoch_{epoch+1}.pt\")\n",
        "        torch.save(model.state_dict(), ckpt_path)\n",
        "\n",
        "        # Generate Samples & Compute Text Metrics\n",
        "        metrics = evaluate_generation(model, model_type, val_loader, config, epoch)\n",
        "        metrics['epoch'] = epoch + 1\n",
        "        metrics['train_loss'] = avg_train_loss\n",
        "        metrics['val_loss'] = val_loss\n",
        "        metrics_log.append(metrics)\n",
        "\n",
        "        # Save logs\n",
        "        pd.DataFrame(metrics_log).to_csv(os.path.join(config.log_dir, f\"{model_type}_metrics.csv\"), index=False)\n",
        "\n",
        "def evaluate_loss(model, model_type, loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            if model_type == \"ARM\":\n",
        "                loss = train_step_arm(model, batch)\n",
        "            else:\n",
        "                loss = train_step_diffusion(model, batch)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad2b2636",
      "metadata": {
        "id": "ad2b2636"
      },
      "source": [
        "### 5. Inference and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e267098",
      "metadata": {
        "id": "9e267098"
      },
      "source": [
        "#### 5.1 Generation Strategy\n",
        "1) ARM: Generates tokens left-to-right (O(N) cost).\n",
        "2) Diffusion: Uses semi-autoregressive block-wise sampling. High-confidence predictions are retained, while low-confidence tokens are re-masked for refinement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea6941c5-757b-4255-8086-f64cfde387f2",
      "metadata": {
        "id": "ea6941c5-757b-4255-8086-f64cfde387f2"
      },
      "outputs": [],
      "source": [
        "def generate_arm(model, prompt_ids, max_new_tokens=50, eos_token_id=None):\n",
        "    \"\"\"\n",
        "    Standard Autoregressive Generation (Greedy).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    curr_ids = prompt_ids.clone()\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        with torch.no_grad():\n",
        "            outputs = model(curr_ids)\n",
        "            # Predict next token from the last position\n",
        "            next_token_logits = outputs[:, -1, :]\n",
        "            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
        "\n",
        "            curr_ids = torch.cat([curr_ids, next_token], dim=1)\n",
        "\n",
        "            # optional: stop if EOS is generated\n",
        "            if eos_token_id is not None and (next_token == eos_token_id).all():\n",
        "                break\n",
        "\n",
        "    return curr_ids\n",
        "\n",
        "def generate_diffusion(model, prompt_ids, target_len=50, steps=20, block_size=None, repetition_penalty=1.5, eos_token_id=None):\n",
        "    \"\"\"\n",
        "    LLaDA-style Generation with:\n",
        "    1. Greedy Decoding (Argmax) for stability\n",
        "    2. Repetition Penalty to reduce stuttering\n",
        "    3. EOS Truncation (stops generating if </s> is found)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Start with the prompt\n",
        "    curr_ids = prompt_ids.clone()\n",
        "    batch_size = curr_ids.shape[0]\n",
        "\n",
        "    # Default to full parallel generation if block_size is not set\n",
        "    if block_size is None or block_size <= 0:\n",
        "        block_size = target_len\n",
        "\n",
        "    # Determine how many blocks we need\n",
        "    num_blocks = math.ceil(target_len / block_size)\n",
        "\n",
        "    for block_idx in range(num_blocks):\n",
        "        # 1. Determine length of this specific block\n",
        "        current_block_len = min(block_size, target_len - (block_idx * block_size))\n",
        "\n",
        "        # 2. Append MASK tokens for the current block ONLY\n",
        "        context_len = curr_ids.shape[1]\n",
        "        mask_token_id = config.mask_token_id\n",
        "\n",
        "        mask_append = torch.full((batch_size, current_block_len), mask_token_id, device=curr_ids.device)\n",
        "        curr_ids = torch.cat([curr_ids, mask_append], dim=1)\n",
        "\n",
        "        # 3. Iterative Denoising\n",
        "        for step in range(steps):\n",
        "            t = 1.0 - ((step + 1) / steps) # Represents time going from 1 to 0\n",
        "            ratio_masked = math.cos(t * math.pi * 0.5) # Apply cosine map\n",
        "\n",
        "            with torch.no_grad():\n",
        "                logits = model(curr_ids)\n",
        "\n",
        "            # Focus only on the current block's logits\n",
        "            block_logits = logits[:, context_len:, :]\n",
        "\n",
        "            # --- Repetition Penalty ---\n",
        "            if repetition_penalty > 1.0:\n",
        "                for i in range(batch_size):\n",
        "                    existing_tokens = curr_ids[i].unique()\n",
        "                    for t_id in existing_tokens:\n",
        "                        if t_id not in [config.pad_token_id, config.sep_token_id, config.mask_token_id]:\n",
        "                            if block_logits[i, :, t_id].max() > 0:\n",
        "                                block_logits[i, :, t_id] /= repetition_penalty\n",
        "                            else:\n",
        "                                block_logits[i, :, t_id] *= repetition_penalty\n",
        "\n",
        "            probs = F.softmax(block_logits, dim=-1)\n",
        "\n",
        "            # --- Greedy Decoding (Argmax) ---\n",
        "            pred_ids = torch.argmax(probs, dim=-1)\n",
        "\n",
        "            # Confidence calculation\n",
        "            confidence = torch.max(probs, dim=-1).values\n",
        "\n",
        "            # Re-masking logic\n",
        "            num_to_mask = int(current_block_len * ratio_masked)\n",
        "\n",
        "            if num_to_mask == 0:\n",
        "                curr_ids[:, context_len:] = pred_ids\n",
        "                break\n",
        "\n",
        "            _, mask_indices = torch.topk(confidence, k=num_to_mask, dim=1, largest=False)\n",
        "            curr_ids[:, context_len:] = pred_ids\n",
        "\n",
        "            batch_indices = torch.arange(batch_size, device=curr_ids.device).unsqueeze(1).expand_as(mask_indices)\n",
        "            current_block_ids = curr_ids[:, context_len:].clone()\n",
        "            current_block_ids[batch_indices, mask_indices] = mask_token_id\n",
        "            curr_ids[:, context_len:] = current_block_ids\n",
        "\n",
        "    # --- EOS Truncation Logic ---\n",
        "    # If eos_token_id is provided, we cut the tensor short\n",
        "    if eos_token_id is not None:\n",
        "        # Check the first sequence in the batch (assuming batch_size=1 for inference)\n",
        "        # Find first occurrence of EOS in the generated part\n",
        "        generated_part = curr_ids[0, prompt_ids.shape[1]:]\n",
        "        eos_indices = (generated_part == eos_token_id).nonzero(as_tuple=True)[0]\n",
        "\n",
        "        if len(eos_indices) > 0:\n",
        "            first_eos = eos_indices[0].item()\n",
        "            # Truncate: Keep prompt + generated up to EOS\n",
        "            curr_ids = curr_ids[:, :prompt_ids.shape[1] + first_eos]\n",
        "\n",
        "    return curr_ids\n",
        "\n",
        "def clean_stutter(text):\n",
        "    \"\"\"\n",
        "    Simple post-processing to remove immediate word repetitions.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    if not words: return \"\"\n",
        "    clean_words = [words[0]]\n",
        "    for w in words[1:]:\n",
        "        if w.lower() != clean_words[-1].lower():\n",
        "            clean_words.append(w)\n",
        "    return \" \".join(clean_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb3ad656",
      "metadata": {
        "id": "fb3ad656"
      },
      "source": [
        "#### 5.2 Evaluation Metrics\n",
        "We evaluate using BLEU, ROUGE, and BERTScore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ef3af0c-9d89-4a8b-952f-ff25f1680e60",
      "metadata": {
        "id": "1ef3af0c-9d89-4a8b-952f-ff25f1680e60"
      },
      "outputs": [],
      "source": [
        "import sacrebleu\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "def calculate_metrics(refs, preds):\n",
        "    # BLEU\n",
        "    bleu = sacrebleu.corpus_bleu(preds, [[r] for r in refs])\n",
        "\n",
        "    # ROUGE\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    rouge_l = np.mean([scorer.score(r, p)['rougeL'].fmeasure for r, p in zip(refs, preds)])\n",
        "\n",
        "    # BERTScore (using distilbert for speed)\n",
        "    try:\n",
        "        P, R, F1 = bert_score(preds, refs, lang=\"en\", verbose=False, model_type=\"distilbert-base-uncased\")\n",
        "        f1_score = F1.mean().item()\n",
        "        p_score = P.mean().item()\n",
        "        r_score = R.mean().item()\n",
        "    except Exception as e:\n",
        "        print(f\"BERTScore failed: {e}\")\n",
        "        f1_score, p_score, r_score = 0.0, 0.0, 0.0\n",
        "\n",
        "    return {\n",
        "        \"bleu\": bleu.score,\n",
        "        \"rouge_l\": rouge_l,\n",
        "        \"bert_p\": p_score,\n",
        "        \"bert_r\": r_score,\n",
        "        \"bert_f1\": f1_score\n",
        "    }\n",
        "\n",
        "def evaluate_generation(model, model_type, loader, config, epoch):\n",
        "    model.eval()\n",
        "    inputs_list = []\n",
        "    refs_list = []\n",
        "    preds_list = []\n",
        "\n",
        "    # Process a few batches only for speed\n",
        "    limit_batches = 5\n",
        "\n",
        "    print(f\"Generating samples for {model_type}...\")\n",
        "    with torch.no_grad():\n",
        "        for i, (input_ids, _) in enumerate(loader):\n",
        "            if i >= limit_batches: break\n",
        "            input_ids = input_ids.to(device)\n",
        "\n",
        "            # Iterate over the batch\n",
        "            for idx in range(input_ids.shape[0]):\n",
        "                row = input_ids[idx]\n",
        "                sep_idx = (row == config.sep_token_id).nonzero()\n",
        "                if len(sep_idx) == 0: continue\n",
        "                sep_pos = sep_idx.item()\n",
        "\n",
        "                # 1. 2D Tensor for the Model (Batch Size = 1)\n",
        "                prompt_ids_model = row[:sep_pos+1].unsqueeze(0)\n",
        "\n",
        "                # 2. 1D Tensor for the Tokenizer (List of IDs)\n",
        "                prompt_ids_decode = row[:sep_pos+1]\n",
        "\n",
        "                target_ids = row[sep_pos+1:]\n",
        "\n",
        "                # Decode using the 1D tensors\n",
        "                ref_text = tokenizer.decode(target_ids, skip_special_tokens=False)\n",
        "                prompt_text = tokenizer.decode(prompt_ids_decode, skip_special_tokens=False)\n",
        "\n",
        "                # Generate\n",
        "                if model_type == \"ARM\":\n",
        "                    # Use the 2D model input\n",
        "                    gen_ids = generate_arm(model, prompt_ids_model)\n",
        "                    new_tokens = gen_ids[0, prompt_ids_model.shape[1]:]\n",
        "                else:\n",
        "                    tgt_len = target_ids.shape[0]\n",
        "                    # Use the 2D model input\n",
        "                    gen_ids = generate_diffusion(model, prompt_ids_model, target_len=tgt_len)\n",
        "\n",
        "                    new_tokens = gen_ids[0, prompt_ids_model.shape[1]:]\n",
        "\n",
        "                pred_text = tokenizer.decode(new_tokens, skip_special_tokens=False)\n",
        "\n",
        "                inputs_list.append(prompt_text)\n",
        "                refs_list.append(ref_text)\n",
        "                preds_list.append(pred_text)\n",
        "\n",
        "    # Save Samples\n",
        "    df_samples = pd.DataFrame({\n",
        "        'epoch': epoch + 1,\n",
        "        'input': inputs_list,\n",
        "        'reference': refs_list,\n",
        "        'prediction': preds_list\n",
        "    })\n",
        "\n",
        "    # Create directory if it doesn't exist just in case\n",
        "    os.makedirs(config.sample_dir, exist_ok=True)\n",
        "\n",
        "    sample_file = os.path.join(config.sample_dir, f\"{model_type}_epoch_{epoch+1}.csv\")\n",
        "    df_samples.head(5).to_csv(sample_file, index=False)\n",
        "    print(f\"Saved samples to {sample_file}\")\n",
        "\n",
        "    # Calculate Metrics\n",
        "    if len(preds_list) > 0:\n",
        "        try:\n",
        "            metrics = calculate_metrics(refs_list, preds_list)\n",
        "            print(f\"Epoch {epoch+1} Metrics: {metrics}\")\n",
        "            return metrics\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping metrics calculation due to error (likely OOM): {e}\")\n",
        "            return {}\n",
        "    return {}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "655895b3",
      "metadata": {
        "id": "655895b3"
      },
      "source": [
        "### Execution and Results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e21d8b2",
      "metadata": {
        "id": "6e21d8b2"
      },
      "source": [
        "#### 6.1 Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5f1b502-b0fe-4faf-8a5a-43457694f94e",
      "metadata": {
        "id": "d5f1b502-b0fe-4faf-8a5a-43457694f94e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Train both models\n",
        "# train_model(model_arm, \"ARM\", train_loader, val_loader, config)\n",
        "# train_model(model_diff, \"Diffusion\", train_loader, val_loader, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76e444fc",
      "metadata": {
        "id": "76e444fc"
      },
      "source": [
        "#### 6.2 Model Loading & Qualitative Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59b419e3",
      "metadata": {
        "id": "59b419e3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import sacrebleu\n",
        "import numpy as np\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bfd7f90",
      "metadata": {
        "id": "5bfd7f90"
      },
      "outputs": [],
      "source": [
        "# --- 1. Configuration & Checkpoint Loading ---\n",
        "# Paths to your saved checkpoints\n",
        "CHECKPOINT_ARM = \"checkpoints/ARM_epoch_40.pt\"       # Adjust epoch as needed\n",
        "CHECKPOINT_DIFF = \"checkpoints/Diffusion_epoch_40.pt\" # Adjust epoch as needed\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"Loading models on {device}...\")\n",
        "\n",
        "# Initialize Architectures (Must match training config)\n",
        "try:\n",
        "    model_arm = ARM(config).to(device)\n",
        "    model_diff = DiffusionLLM(config).to(device)\n",
        "except NameError:\n",
        "    raise NameError(\"Model classes (ARM, Diffusion) not defined. Please run the Architecture cell first.\")\n",
        "\n",
        "# Load ARM State\n",
        "if os.path.exists(CHECKPOINT_ARM):\n",
        "    state_dict_arm = torch.load(CHECKPOINT_ARM, map_location=device)\n",
        "    model_arm.load_state_dict(state_dict_arm)\n",
        "    model_arm.eval()\n",
        "    print(f\"✅ ARM loaded from {CHECKPOINT_ARM}\")\n",
        "else:\n",
        "    print(f\"❌ ARM checkpoint not found at {CHECKPOINT_ARM}\")\n",
        "\n",
        "# Load Diffusion State\n",
        "if os.path.exists(CHECKPOINT_DIFF):\n",
        "    state_dict_diff = torch.load(CHECKPOINT_DIFF, map_location=device)\n",
        "    model_diff.load_state_dict(state_dict_diff)\n",
        "    model_diff.eval()\n",
        "    print(f\"✅ Diffusion loaded from {CHECKPOINT_DIFF}\")\n",
        "else:\n",
        "    print(f\"❌ Diffusion checkpoint not found at {CHECKPOINT_DIFF}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a7520e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- 2. Unified Translation Wrapper ---\n",
        "\n",
        "def translate_unified(text, model, model_type, penalty=1.5):\n",
        "    \"\"\"\n",
        "    Unified inference wrapper for both ARM and Diffusion models.\n",
        "    \"\"\"\n",
        "    # Common Tokenization\n",
        "    input_ids = tokenizer.encode(text)\n",
        "    \n",
        "    if model_type == \"ARM\":\n",
        "        # ARM Input: Just the prompt IDs\n",
        "        input_tensor = torch.tensor([input_ids], device=device)\n",
        "        \n",
        "        # Standard Greedy Generation\n",
        "        gen_ids = generate_arm(\n",
        "            model, \n",
        "            input_tensor, \n",
        "            max_new_tokens=50, \n",
        "            eos_token_id=config.eos_token_id\n",
        "        )\n",
        "        \n",
        "        # Slice: Remove the prompt to get only the translation\n",
        "        new_tokens = gen_ids[0, len(input_ids):]\n",
        "        \n",
        "        # Decode\n",
        "        raw_output = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "        return raw_output.strip()\n",
        "\n",
        "    elif model_type == \"Diffusion\":\n",
        "        # Diffusion Input: Prompt + SEP token\n",
        "        input_tensor = torch.tensor([input_ids + [config.sep_token_id]], device=device)\n",
        "        \n",
        "        # Dynamic Target Length: French is ~1.3x English length\n",
        "        tgt_len = int(len(input_ids) * 1.3) + 2\n",
        "        \n",
        "        # LLaDA-style Generation\n",
        "        gen_ids = generate_diffusion(\n",
        "            model, \n",
        "            input_tensor, \n",
        "            target_len=tgt_len, \n",
        "            steps=50, \n",
        "            repetition_penalty=penalty,\n",
        "            eos_token_id=config.eos_token_id \n",
        "        )\n",
        "        \n",
        "        # Slice: Find SEP and take everything after\n",
        "        try:\n",
        "            sep_loc = (gen_ids[0] == config.sep_token_id).nonzero(as_tuple=True)[0].item()\n",
        "            new_tokens = gen_ids[0, sep_loc+1:]\n",
        "            \n",
        "            # Manual EOS truncation check\n",
        "            if config.eos_token_id in new_tokens:\n",
        "                end_loc = (new_tokens == config.eos_token_id).nonzero(as_tuple=True)[0][0].item()\n",
        "                new_tokens = new_tokens[:end_loc]\n",
        "                \n",
        "            raw_output = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "            \n",
        "            # Uses clean_stutter from the previous cell\n",
        "            return clean_stutter(raw_output).strip()\n",
        "            \n",
        "        except Exception as e:\n",
        "            return f\"[Error in generation: {e}]\"\n",
        "    \n",
        "    return \"[Unknown Model Type]\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf8ee46a",
      "metadata": {
        "id": "bf8ee46a"
      },
      "outputs": [],
      "source": [
        "import sacrebleu\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score\n",
        "\n",
        "# ==========================================\n",
        "# 1. SETUP: Ladder of Difficulty Dataset\n",
        "# ==========================================\n",
        "test_data = [\n",
        "    # --- Level 1: Basics ---\n",
        "    (\"The sun is shining.\", \"Le soleil brille.\"),\n",
        "    (\"He opened the door.\", \"Il a ouvert la porte.\"),\n",
        "    (\"She looked at him.\", \"Elle le regarda.\"),\n",
        "    (\"I am happy.\", \"Je suis heureux.\"),\n",
        "    (\"The dog runs fast.\", \"Le chien court vite.\"),\n",
        "    (\"The cat sleeps on the bed.\", \"Le chat dort sur le lit.\"),\n",
        "    (\"We eat apples.\", \"Nous mangeons des pommes.\"),\n",
        "    (\"They drink water.\", \"Ils boivent de l'eau.\"),\n",
        "    (\"My father is tall.\", \"Mon père est grand.\"),\n",
        "    (\"The car is red.\", \"La voiture est rouge.\"),\n",
        "\n",
        "    # --- Level 2: Simple Grammar ---\n",
        "    (\"I do not know the answer.\", \"Je ne connais pas la réponse.\"),\n",
        "    (\"Where is the house?\", \"Où est la maison ?\"),\n",
        "    (\"The little boy played in the garden.\", \"Le petit garçon jouait dans le jardin.\"),\n",
        "    (\"She is a very beautiful woman.\", \"C'est une très belle femme.\"),\n",
        "    (\"He said nothing to me.\", \"Il ne m'a rien dit.\"),\n",
        "    (\"Why are you sad?\", \"Pourquoi es-tu triste ?\"),\n",
        "    (\"I did not see the movie.\", \"Je n'ai pas vu le film.\"),\n",
        "    (\"Do you have a pen?\", \"As-tu un stylo ?\"),\n",
        "    (\"The big house is old.\", \"La grande maison est vieille.\"),\n",
        "    (\"We are not ready yet.\", \"Nous ne sommes pas encore prêts.\"),\n",
        "\n",
        "    # --- Level 3: Compound Sentences ---\n",
        "    (\"I went to the market and I bought some bread.\", \"Je suis allé au marché et j'ai acheté du pain.\"),\n",
        "    (\"He wanted to come, but he was too tired.\", \"Il voulait venir, mais il était trop fatigué.\"),\n",
        "    (\"If it rains, we will stay inside.\", \"S'il pleut, nous resterons à l'intérieur.\"),\n",
        "    (\"She told me that she loved him.\", \"Elle m'a dit qu'elle l'aimait.\"),\n",
        "    (\"When I arrived, the door was already open.\", \"Quand je suis arrivé, la porte était déjà ouverte.\"),\n",
        "    (\"I called him but he did not answer.\", \"Je l'ai appelé mais il n'a pas répondu.\"),\n",
        "    (\"She was reading while I was cooking.\", \"Elle lisait pendant que je cuisinais.\"),\n",
        "    (\"You can stay or you can go.\", \"Tu peux rester ou tu peux partir.\"),\n",
        "    (\"Because it was late, we went home.\", \"Comme il était tard, nous sommes rentrés à la maison.\"),\n",
        "    (\"He thinks that we are lost.\", \"Il pense que nous sommes perdus.\"),\n",
        "\n",
        "    # --- Level 4: Complex Grammar ---\n",
        "    (\"The man who is standing there is my brother.\", \"L'homme qui se tient là est mon frère.\"),\n",
        "    (\"It is necessary that you leave immediately.\", \"Il est nécessaire que vous partiez immédiatement.\"),\n",
        "    (\"Despite the rain, they continued their journey.\", \"Malgré la pluie, ils ont continué leur voyage.\"),\n",
        "    (\"She wondered if he would ever return to this place.\", \"Elle se demandait s'il reviendrait un jour à cet endroit.\"),\n",
        "    (\"He spoke with a voice that was both calm and terrifying.\", \"Il parlait d'une voix à la fois calme et terrifiante.\"),\n",
        "    (\"The book that I read yesterday was boring.\", \"Le livre que j'ai lu hier était ennuyeux.\"),\n",
        "    (\"I doubt that he is telling the truth.\", \"Je doute qu'il dise la vérité.\"),\n",
        "    (\"She asked me where I had bought this coat.\", \"Elle m'a demandé où j'avais acheté ce manteau.\"),\n",
        "    (\"Before you leave, please close the window.\", \"Avant que tu partes, s'il te plaît ferme la fenêtre.\"),\n",
        "    (\"The woman whose car was stolen is crying.\", \"La femme dont la voiture a été volée pleure.\"),\n",
        "\n",
        "    # --- Level 5: Literary/Abstract ---\n",
        "    (\"A deep silence reigned in the room, broken only by the ticking of the clock.\", \"Un grand silence régnait dans la pièce, troublé seulement par le tic-tac de l'horloge.\"),\n",
        "    (\"He felt a strange sensation, as if someone were watching him from the shadows.\", \"Il éprouvait une étrange sensation, comme si quelqu'un l'observait depuis l'ombre.\"),\n",
        "    (\"It was the best of times, it was the worst of times.\", \"C'était le meilleur des temps, c'était le pire des temps.\"),\n",
        "    (\"Suddenly, a loud cry rang out through the night, freezing his blood.\", \"Soudain, un grand cri retentit dans la nuit, lui glaçant le sang.\"),\n",
        "    (\"She sat by the window, watching the dead leaves fall slowly to the ground.\", \"Elle était assise près de la fenêtre, regardant les feuilles mortes tomber lentement vers le sol.\"),\n",
        "    (\"The wind howled through the trees like a wounded animal.\", \"Le vent hurlait à travers les arbres comme un animal blessé.\"),\n",
        "    (\"In the distance, the mountains rose sharply against the pale sky.\", \"Au loin, les montagnes se dressaient brusquement contre le ciel pâle.\"),\n",
        "    (\"He knew in his heart that this was the end of the journey.\", \"Il savait dans son cœur que c'était la fin du voyage.\"),\n",
        "    (\"The ancient castle stood silent, guarding its secrets for centuries.\", \"L'ancien château se tenait silencieux, gardant ses secrets depuis des siècles.\"),\n",
        "    (\"A heavy fog covered the city, hiding the streets from view.\", \"Un brouillard épais couvrait la ville, dérobant les rues à la vue.\"),\n",
        "]\n",
        "\n",
        "# ==========================================\n",
        "# 2. EVALUATION LOOP (Unified)\n",
        "# ==========================================\n",
        "references = []\n",
        "predictions_arm = []\n",
        "predictions_diff = []\n",
        "\n",
        "print(f\"{'='*20} RUNNING INFERENCE ON {len(test_data)} SENTENCES {'='*20}\")\n",
        "\n",
        "for i, (source, ref) in enumerate(test_data):\n",
        "    # 1. ARM Generation\n",
        "    pred_arm = translate_unified(source, model_arm, \"ARM\")\n",
        "    if not pred_arm.strip(): pred_arm = \".\"\n",
        "\n",
        "    # 2. Diffusion Generation (with penalty)\n",
        "    pred_diff = translate_unified(source, model_diff, \"Diffusion\", penalty=1.5)\n",
        "    if not pred_diff.strip(): pred_diff = \".\"\n",
        "\n",
        "    predictions_arm.append(pred_arm)\n",
        "    predictions_diff.append(pred_diff)\n",
        "    references.append(ref)\n",
        "\n",
        "    if (i + 1) % 10 == 0:\n",
        "        print(f\"Processed {i + 1} sentences...\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. HELPER: Metric Calculator\n",
        "# ==========================================\n",
        "def get_metrics(preds, refs, model_name):\n",
        "    print(f\"\\nCalculating metrics for {model_name}...\")\n",
        "\n",
        "    # BLEU\n",
        "    bleu = sacrebleu.corpus_bleu(preds, [[r] for r in refs])\n",
        "\n",
        "    # ROUGE\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    rouge1, rougeL = [], []\n",
        "    for r, p in zip(refs, preds):\n",
        "        scores = scorer.score(r, p)\n",
        "        rouge1.append(scores['rouge1'].fmeasure)\n",
        "        rougeL.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "    # BERTScore\n",
        "    try:\n",
        "        P, R, F1 = bert_score(preds, refs, lang=\"fr\", verbose=False)\n",
        "        bert_f1 = F1.mean().item()\n",
        "    except Exception as e:\n",
        "        print(f\"BERTScore warning: {e}\")\n",
        "        bert_f1 = 0.0\n",
        "\n",
        "    return {\n",
        "        \"BLEU\": bleu.score,\n",
        "        \"ROUGE-1\": np.mean(rouge1),\n",
        "        \"ROUGE-L\": np.mean(rougeL),\n",
        "        \"BERTScore\": bert_f1\n",
        "    }\n",
        "\n",
        "# ==========================================\n",
        "# 4. COMPUTE METRICS\n",
        "# ==========================================\n",
        "metrics_arm = get_metrics(predictions_arm, references, \"ARM\")\n",
        "metrics_diff = get_metrics(predictions_diff, references, \"Diffusion\")\n",
        "\n",
        "# ==========================================\n",
        "# 5. FINAL REPORT & COMPARISON\n",
        "# ==========================================\n",
        "\n",
        "# A. Metrics Table\n",
        "results_df = pd.DataFrame([metrics_diff, metrics_arm], index=[\"Diffusion\", \"ARM\"])\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"       FINAL PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*40)\n",
        "print(results_df.round(4))\n",
        "print(\"=\"*40 + \"\\n\")\n",
        "\n",
        "# B. Detailed List Display\n",
        "print(f\"{'='*20} DETAILED GENERATION LOG {'='*20}\")\n",
        "for i, (src, ref) in enumerate(test_data):\n",
        "    print(f\"[{i+1}] Input:      {src}\")\n",
        "    print(f\"    Reference:  {ref}\")\n",
        "    print(f\"    ARM:        {predictions_arm[i]}\")\n",
        "    print(f\"    Diffusion:  {predictions_diff[i]}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce49c4a2",
      "metadata": {
        "id": "ce49c4a2"
      },
      "source": [
        "# 6. Conclusions and Future Work\n",
        "\n",
        "## **6.1 Summary of Findings**\n",
        "This work presented a systematic comparison between Autoregressive Models (ARM) and Masked Diffusion Models (MDM) for English-to-French machine translation using a 44M parameter scale.\n",
        "\n",
        "Our experiments demonstrate that **autoregressive models substantially outperform diffusion models** in this specific setting:\n",
        "* **Quality:** The ARM achieved a **BLEU score of 30.42**, which is approximately **2.2× higher** than the diffusion model's score of 13.77.\n",
        "* **Coherence:** Qualitative analysis shows the ARM produces fluent, grammatically correct translations, whereas the diffusion model struggles with long-range dependencies and occasionally introduces repetition artifacts.\n",
        "* **Efficiency:** The autoregressive approach converges significantly faster, reaching lower loss values within the 50-epoch constraint.\n",
        "\n",
        "## **6.2 Limitations and Analysis**\n",
        "While the autoregressive model is superior at this scale, these results likely reflect **training efficiency differences** rather than fundamental algorithmic limitations of diffusion.\n",
        "* **Scale:** Diffusion models often require larger parameter scales (e.g., 7B+) for emergent capabilities like global planning to manifest.\n",
        "* **Training Time:** Diffusion models typically require 5–10× more training steps to reach parity with ARMs, which our 50-epoch limit did not permit.\n",
        "* **Architecture:** Our diffusion implementation lacked advanced features like Rotary Positional Embeddings (RoPE) or Grouped-Query Attention (GQA), which are standard in state-of-the-art models like LLaDA.\n",
        "\n",
        "## **6.3 Future Work**\n",
        "To fully realize the potential of diffusion models for text generation, future research should focus on:\n",
        "1.  **Scaling Experiments:** Training at 1B+ parameters to unlock emergent bidirectional reasoning capabilities.\n",
        "2.  **Extended Training Schedules:** Increasing the training budget to allow the diffusion model to fully converge.\n",
        "3.  **Advanced Architectures:** Incorporating modern techniques such as RoPE and GQA to better exploit bidirectional context.\n",
        "4.  **Task Diversity:** Evaluating performance on tasks that inherently benefit from non-sequential generation, such as text infilling, constrained generation, or iterative revision."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
