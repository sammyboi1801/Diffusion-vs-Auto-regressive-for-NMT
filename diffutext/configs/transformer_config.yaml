# Configuration for the Transformer Model

model_params:
  tokenizer_name: 'bert-base-uncased' # Example tokenizer
  vocab_size: 30522
  d_model: 512
  nhead: 8
  num_encoder_layers: 6
  num_decoder_layers: 6
  dim_feedforward: 2048
  dropout: 0.1

data:
  path: 'path/to/your/dataset'
  format: 'text' # or 'json', 'csv', etc.

training:
  batch_size: 32
  num_epochs: 20
  learning_rate: 0.0001
  optimizer: 'AdamW'
  scheduler: 'None'

inference:
  max_length: 100
  num_beams: 5

evaluation:
  batch_size: 64
  metric: 'bleu'
